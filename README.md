# LibrAIry  
**Retrieval-Augmented Reading Assistant for Your Personal e-Library**

[![Build](https://img.shields.io/github/actions/workflow/status/your-org/librairy/ci.yml?branch=main)](../../actions)  
[![Docker](https://img.shields.io/badge/docker-ready-to-run-2496ED?logo=docker&logoColor=white)](https://hub.docker.com/u/your-org)  
[![License](https://img.shields.io/github/license/your-org/librairy)](LICENSE)

---

> **En deux mots** : LibrAIry indexe tous vos PDF/EPUB libres dâ€™accÃ¨s (hÃ©bergÃ©s sur votre espace Mega, un NAS ou simplement votre disque) et vous offre un
> **chatbot contextuel**, des **rÃ©sumÃ©s**, des **flashcards** et mÃªme un mode
> **lecture audio** â€“ le tout propulsÃ© par un pipeline RAG de pointe.

---

## âœ¨ FonctionnalitÃ©s principales

| Fonction | DÃ©tails techniques | Valeur utilisateur |
|-----------|-------------------|--------------------|
| **Chat universel** | RAG â€¢ Llama 3/Mixtral â€¢ bge-reranker | Posez nâ€™importe quelle question, recevez une rÃ©ponse sourcÃ©e paragraphe + page. |
| **Recherche sÃ©mantique** | Embeddings Instructor-XL â€¢ Qdrant | Trouvez un concept mÃªme si les mots exacts diffÃ¨rent. |
| **RÃ©sumÃ©s & plans** | Summarisation (Llama 3) | SynthÃ¨se par chapitre ou par ouvrage complet. |
| **Flashcards automatiques** | LLM chain â†’ format Anki `.apkg` | RÃ©visez efficacement et exportez vers AnkiMobile. |
| **Lecture audio TTS** | Bark / XTTS | Convertit un extrait en podcast (MP3). |
| **Tableau de bord dâ€™apprentissage** | TimescaleDB â€¢ Grafana | Suivi du temps de lecture, thÃ¨mes, progression. |
| **Recommandations croisÃ©es** | Classification + similarity | â€œSi ce chapitre vous a plu, lisez aussiâ€¦â€ |

---

## ğŸ§ Pourquoi LibrAIry ?

* **DÃ©niche lâ€™info enfouie** dans des milliers de pages PDF.
* Fonctionne **entiÃ¨rement hors-ligne** (utile pour contenu sensible).
* **Project-portfolio** complet : ingestion â†’ vecteur DB â†’ RAG â†’ UI â†’ MLOps.
* Combine **Python pour le ML** et **Rust pour la performance**.

---

## ğŸ—ï¸ Architecture

          +-------------+
```
User Chat â‡†   | FastAPI API |  â† Auth, Rate-limit
+â€”â€”+â€”â€”+
|
+â€”â€”â€”â€“+â€”â€”â€”â€“+
|  RAG Orchestrator     |  (LangChain / LlamaIndex)
+â€”â€”â€”â€“+â€”â€”â€”â€“+
|
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€Retrieveâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector DB (Qdrant)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ embeddings
PDF/EPUB   â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Ingestion Pipeline (Python)
â”œâ”€ PyMuPDF / Tika         # extraction texte
â”œâ”€ OCR (Tesseract)        # pages scannÃ©es
â””â”€ Embeddings (Instructor-XL)

After Retrieve
â–¼
Re-rank (bge-reranker) â†’  Context Compressor  â†’  LLM (Llama 3)
â”‚
Summaries / Flashcards / TTS

*Toutes les briques sont containerisÃ©es ; orchestration : **Docker Compose** (dev) ou **Helm/k3s** (prod).*
```
---

## ğŸ§° Stack technologique

| Couche | Choix | Raisons |
|--------|-------|---------|
| **Ingestion** | Python 3.11 â€¢ PyMuPDF â€¢ pdfminer.six â€¢ unstructured â€¢ Tesseract-OCR | Extraction fiable, multi-langue. |
| **Embeddings** | `sentence-transformers/instructor-xl` (GGUF quantisÃ©) | Excellente perf multilingue. |
| **Vector Store** | **Qdrant** (Docker) | API gRPC & REST, scale horizontale. |
| **RAG Framework** | **LangChain** ou **LlamaIndex** | Graph dâ€™outils flexible. |
| **LLM** | Llama 3 8B Instruct (gguf Q4) via `llama.cpp-server` (Rust) | Latence <1 s sur GPU moyen ou MPS. |
| **Backend API** | **FastAPI** | Docs auto (Swagger), WebSocket facile. |
| **Service haute perf** | **Rust/axum** â€“ streaming TTS & gros download | Apprentissage Rust + dÃ©bit. |
| **Frontend** | **Next.js 14** (React Server) â€¢ Tailwind â€¢ shadcn/ui | SSR performant, dark/light. |
| **MLOps** | MLflow â€¢ DVC â€¢ Prefect â€¢ Prometheus â€¢ Grafana | TraÃ§abilitÃ©, monitoring, pipelines planifiÃ©s. |
| **CI/CD** | GitHub Actions â€¢ Docker Buildx â€¢ Helm | Ligne verte jusquâ€™Ã  k3s. |

---

## âš¡ Mise en route rapide

### 1. PrÃ©requis

| Outil | Version minimale |
|-------|------------------|
| Docker / Compose | 24 + (compose v2) |
| Python | 3.11 |
| Node.js | 20 |
| Rust | 1.76 |
| Tesseract OCR | 5.3 |
| (Option) k3d/k3s | 1.29 |

### 2. Clone & quickstart

```bash
git clone https://github.com/your-org/librairy.git
cd librairy
make quickstart           # build & docker-compose up -d
open http://localhost:3000  # UI Next.js
```
Le script make quickstart :
	1.	tÃ©lÃ©charge/installe les modÃ¨les (gguf, bark).
	2.	crÃ©e les conteneurs Postgres, Qdrant, FastAPI, llama.cpp-server.
	3.	expose lâ€™UI sur localhost:3000.

3. Indexer vos livres
	1.	DÃ©posez vos PDF/EPUB/CBZ dans data/books/.
	2.	ExÃ©cutez : ``` python ingest/run_pipeline.py --path data/books ```
	3.	Surveillez lâ€™avancement dans prefect Orion (http://localhost:4200).

Tips : gros corpus ? Ajoutez lâ€™option --chunk 100 pour batcher les
embeddings et Ã©viter lâ€™OOM.

4. Tester lâ€™API
```bash
curl -X POST http://localhost:8000/chat \
     -d '{"query": "Explique la diffÃ©rence entre monades et applicatives"}'
```

â¸»

ğŸ“‚ Structure du repo
```
librairy/
â”œâ”€â”€ ingest/                # pipelines Prefect + scripts d'extraction
â”œâ”€â”€ rag/                   # orchestrateur LangChain
â”œâ”€â”€ api/                   # FastAPI routes
â”œâ”€â”€ rust-services/
â”‚   â””â”€â”€ tts-stream/        # service Rust axum + Bark
â”œâ”€â”€ frontend/              # Next.js 14 app
â”œâ”€â”€ deployments/
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â””â”€â”€ helm/
â”œâ”€â”€ mlops/
â”‚   â”œâ”€â”€ mlruns/            # MLflow
â”‚   â””â”€â”€ dvc.yaml
â””â”€â”€ .github/workflows/     # CI/CD
```

â¸»

ğŸ” ObservabilitÃ©
	â€¢	Prometheus : mÃ©triques rag_latency_seconds, query_error_total.
	â€¢	Grafana : dashboards prÃªts dans dashboards/.
	â€¢	OpenTelemetry : traces end-to-end (optionnel).
	â€¢	Sentry : erreurs front & back.

â¸»

ğŸ—“ï¸ Roadmap 8 semaines

Semaine	Livrable
1	Docker skeleton + Qdrant + FastAPI alive.
2	Pipeline dâ€™extraction & embeddings (PDF + OCR).
3	Endpoint RAG POC (retrieve + LLM).
4	Re-ranker + citations prÃ©cises.
5	UI chat + file manager + surlignage page.
6	Flashcards Anki + service TTS streaming.
7	Dashboard apprentissage (Grafana) + stats.
8	MLOps, CI/CD, doc complÃ¨te, dÃ©mo vidÃ©o.


â¸»

ğŸ”’ Licences & Ã©thique
	â€¢	Contenu : assurez-vous que vos ouvrages sont dans le domaine public ou
sous licence Creative Commons.
	â€¢	ModÃ¨les : Llama 3 est CC-BY-SA ; MiDaS CC-BY-NC, etc. VÃ©rifiez avant mise
en prod commerciale.
	â€¢	Usage : LibrAIry ne stocke pas vos PDF hors de votre infra â€“ pratique pour
protÃ©ger du contenu sensible.

â¸»

ğŸ¤ Contribuer
	1.	Fork, git clone, make dev-setup (installe pre-commit).
	2.	CrÃ©ez une branche feat/ma-feature.
	3.	Commits style Conventional Commits.
	4.	Ouvrez un PR, laissez les tests CI passer.

Voir CONTRIBUTING.md & CODE_OF_CONDUCT.md.

â¸»

ğŸ“œ Licence

DistribuÃ© sous licence MIT â€“ voir LICENSE.

â¸»

ğŸ™ Remerciements
	â€¢	Hugging Face community pour les modÃ¨les open-weight.
	â€¢	Projets LangChain et LlamaIndex pour le tooling RAG.
	â€¢	Qdrant, Prefect, Grafana pour leurs outils open-source formidables.

â¸»

ğŸ“« Contact

Support	Lien
Email	dev@your-domain.com
Twitter	@YourHandle
Discord	discord.gg/abcdefg

Bon hacking et bonnes lectures ! ğŸ“š

